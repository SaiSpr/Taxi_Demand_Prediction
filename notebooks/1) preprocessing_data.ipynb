{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2KQNMxbmMYGF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "V9Os-CMNMYGH",
    "outputId": "4a831379-aaf1-4724-a215-d07b880e18e7"
   },
   "outputs": [],
   "source": [
    "#!pip install pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwAihnrkMYGI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MoL5M_jMMYGI",
    "outputId": "48b08df4-09a3-4593-ef44-05c7dfdcdfb5"
   },
   "outputs": [],
   "source": [
    "#pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tVubXK2dMYGI"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastparquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpq\u001b[39;00m  \u001b[38;5;66;03m# If using pyarrow\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfastparquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfp\u001b[39;00m  \u001b[38;5;66;03m# If using fastparquet\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fastparquet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq  # If using pyarrow\n",
    "\n",
    "import pandas as pd\n",
    "import fastparquet as fp  # If using fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NdkYmJAMYGJ"
   },
   "outputs": [],
   "source": [
    "rides_jan = pd.read_parquet('/Users/chetanhalai/Documents/code base for projects/4) uber/FINAL TAXI PROJECT 2023/data/2023 - JAN/yellow_tripdata_2023-01.parquet')\n",
    "rides_feb = pd.read_parquet('/Users/chetanhalai/Documents/code base for projects/4) uber/FINAL TAXI PROJECT 2023/data/2023 - FEB/yellow_tripdata_2023-02.parquet')\n",
    "rides_mar = pd.read_parquet('/Users/chetanhalai/Documents/code base for projects/4) uber/FINAL TAXI PROJECT 2023/data/2023 - MAR/yellow_tripdata_2023-03.parquet')\n",
    "rides_apr = pd.read_parquet('/Users/chetanhalai/Documents/code base for projects/4) uber/FINAL TAXI PROJECT 2023/data/2023 - APR/yellow_tripdata_2023-04.parquet')\n",
    "rides_may = pd.read_parquet('/Users/chetanhalai/Documents/code base for projects/4) uber/FINAL TAXI PROJECT 2023/data/2023 - MAY/yellow_tripdata_2023-05.parquet')\n",
    "rides_jun = pd.read_parquet('/Users/chetanhalai/Documents/code base for projects/4) uber/FINAL TAXI PROJECT 2023/data/2023 - JUN/yellow_tripdata_2023-06.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wuwkz7IiMYGK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6DTWj0LMYGK",
    "outputId": "08faea16-3d7b-411d-e05e-f7d897f8d833"
   },
   "outputs": [],
   "source": [
    "print(\"January month data shape:\", rides_jan.shape)\n",
    "print(\"February month data shape:\", rides_feb.shape)\n",
    "print(\"March month data shape:\", rides_mar.shape)\n",
    "print(\"April month data shape:\", rides_apr.shape)\n",
    "print(\"May month data shape:\", rides_may.shape)\n",
    "print(\"June month data shape:\", rides_jun.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6pJWUshMYGK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUUJLhwrMYGL"
   },
   "source": [
    "### Preprocess for January - (will do this for every month and combine during visuallization(EDA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwCFH5SmMYGM",
    "outputId": "ecc1fb94-25f7-444a-a317-fad387c38361"
   },
   "outputs": [],
   "source": [
    "rides_jan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QwYWaaO5MYGM",
    "outputId": "f5549d6b-3f61-427b-a047-fc754472cfcd"
   },
   "outputs": [],
   "source": [
    "rides_jan.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ShfSqvWGMYGM",
    "outputId": "d70f76d2-1a36-4801-f920-e55297221bc0"
   },
   "outputs": [],
   "source": [
    "rides_jan.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4178HAkaMYGN"
   },
   "source": [
    "\n",
    "* Despite there being 19 columns the colums i will be focusing on will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w24v1LaUMYGN"
   },
   "outputs": [],
   "source": [
    "rides_jan = rides_jan [['tpep_pickup_datetime', 'PULocationID' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPz5_zI1MYGN"
   },
   "outputs": [],
   "source": [
    "rides_jan.rename(columns={'tpep_pickup_datetime': 'pickup_datetime',\n",
    "                      'PULocationID': 'pickup_location_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EyCUT5KMYGN",
    "outputId": "0f2d3a03-5e29-4191-fc41-14e2fee37acb"
   },
   "outputs": [],
   "source": [
    "rides_jan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqn3IttoMYGN",
    "outputId": "fef56595-c480-4fb7-db73-e4f9d90000d6"
   },
   "outputs": [],
   "source": [
    "#isolated the data that falls between 2023-01-01 and 2023-02-01'\n",
    "rides_jan = rides_jan[rides_jan.pickup_datetime >= '2023-01-01']\n",
    "rides_jan = rides_jan[rides_jan.pickup_datetime < '2023-02-01']\n",
    "rides_jan ['pickup_datetime'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52aUiSrSMYGO",
    "outputId": "312b73ab-4075-411c-e5d3-513902842cfe"
   },
   "outputs": [],
   "source": [
    "rides_jan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8H5LVVbMYGO"
   },
   "outputs": [],
   "source": [
    "#Rounded data to the hour and will work with data in hourly intervals\n",
    "rides_jan['pickup_hour'] = rides_jan['pickup_datetime'].dt.floor('H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8H6GVILMYGO",
    "outputId": "153e1a94-01f6-4353-caa8-69cab029f85d"
   },
   "outputs": [],
   "source": [
    "#going to group location ID and and pick up hour - (.size will give me the count of ride per hour - .reset_index goives me the data in ascending order)\n",
    "agg_rides_jan = rides_jan.groupby(['pickup_hour', 'pickup_location_id']).size().reset_index()\n",
    "agg_rides_jan.rename(columns={0: 'number_of_rides'}, inplace=True) #inplace=True is permenant\n",
    "agg_rides_jan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSewyrm6MYGP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_rFN5zOMYGP",
    "outputId": "c98cbbd3-d73b-44fa-a10a-862ccb12e5a2"
   },
   "outputs": [],
   "source": [
    "agg_rides_jan.shape # i want make surei  have a complete time series - the data above appears to have eliminated rows that have no rides. What has occiures is 'pcikup hours' and 'location id' that had no ride were eliminiate from the data\n",
    "#there is no row - I want to add rows that had no rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMrMD7ztMYGP"
   },
   "outputs": [],
   "source": [
    "#this functions purpose is to fill in the data that was not present in the original data because there where no rides - time series data requires ther to be data in every time slot regardless of whether it existed or not\n",
    "from tqdm import tqdm\n",
    "\n",
    "def add_missing_slots(agg_rides: pd.DataFrame) -> pd.DataFrame: #take in the aggressed dataset that i presievely made and create an output a new DF which is going to be a complete timeseries\n",
    "\n",
    "    location_ids = agg_rides['pickup_location_id'].unique() # extract all the location ids and the full range of dates (bellow)...\n",
    "    full_range = pd.date_range(\n",
    "        agg_rides['pickup_hour'].min(), agg_rides['pickup_hour'].max(), freq='H') # ... from the first date till the last date and then building a series hour by hour\n",
    "    output = pd.DataFrame()\n",
    "    for location_id in tqdm(location_ids): #I did this operation per location ID .....\n",
    "\n",
    "        # keep only rides for this 'location_id'\n",
    "        agg_rides_i = agg_rides.loc[agg_rides.pickup_location_id == location_id, ['pickup_hour', 'number_of_rides']] #.... so i filter the data for onlyt that location ID and then we do this operatiom\n",
    "\n",
    "        # quick way to add missing dates with 0 in a Series\n",
    "        # taken from https://stackoverflow.com/a/19324591\n",
    "        agg_rides_i.set_index('pickup_hour', inplace=True) #this is the Reindex operation which basically takes in a df, (agg_rides_i), and then a full range of dates i want to have ...\n",
    "        agg_rides_i.index = pd.DatetimeIndex(agg_rides_i.index) #..... to ensure the out put df, has as many rows(and dates ) as the full range ......\n",
    "        agg_rides_i = agg_rides_i.reindex(full_range, fill_value=0) #... if thats not tne case it will fill it by 0)\n",
    "\n",
    "        # add back `location_id` columns\n",
    "        agg_rides_i['pickup_location_id'] = location_id #I then add back the location ID in this final dataframe\n",
    "\n",
    "        output = pd.concat([output, agg_rides_i]) # the i concatinate the results with the ones i got from the previous iterations and loop location id by location id\n",
    "\n",
    "    # move the purchase_day from the index to a dataframe column\n",
    "    output = output.reset_index().rename(columns={'index': 'pickup_hour'}) #then create a new column name, so this is why we need to add this rename column to inert thje index\n",
    "                                                                #which is now the datetime as a column and then rename it and its the pickup_hour\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stL5jQ8hMYGP",
    "outputId": "f53e24aa-36be-4587-c78c-cc918e0af6a5"
   },
   "outputs": [],
   "source": [
    "print(agg_rides_jan.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MoXK5H1xMYGP",
    "outputId": "bfe16186-8b78-430d-f9a1-21cb73a314dc"
   },
   "outputs": [],
   "source": [
    "#calling the function and adding the missing slots\n",
    "agg_rides_all_slots_jan = add_missing_slots(agg_rides_jan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dM39ncFMMYGQ",
    "outputId": "f7a5be07-a05f-4041-de9e-2e5012ccb175"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install plotly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7anR_Gf4MYGQ",
    "outputId": "7074c689-7646-4513-94ad-a747cdf2a843"
   },
   "outputs": [],
   "source": [
    "pip install plotly==5.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "haYKy_F2MYGQ"
   },
   "outputs": [],
   "source": [
    "#visualise the data to detect bigs in my data processing\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F3rd5Q-AMYGQ"
   },
   "outputs": [],
   "source": [
    "#visualise the data to detect bigs in my data processing - check if the graph has any breaks in it\n",
    "from typing import Optional, List\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_rides(\n",
    "    rides: pd.DataFrame,\n",
    "    locations: Optional[List[int]] = None\n",
    "    ):     #this function takes in the dataframe i made previously (rides) and the optionally takes in the locations, the ones that will be plotted\n",
    "    \"\"\"\"\n",
    "    Plot time-series data\n",
    "    \"\"\"\n",
    "    rides_to_plot = rides[rides.pickup_location_id.isin(locations)] if locations else rides #once i filtered the data frame I just call the plotly dot line function and it will plot multiple lines\n",
    "\n",
    "    fig = px.line(\n",
    "        rides_to_plot,\n",
    "        x=\"pickup_hour\",\n",
    "        y=\"number_of_rides\",\n",
    "        color='pickup_location_id',\n",
    "        template='none',\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBG-839UMYGQ",
    "outputId": "70e2bdb6-52da-454e-afbd-a58a425186c9"
   },
   "outputs": [],
   "source": [
    "plot_rides(agg_rides_all_slots_jan, locations=[261])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6B1ltv1ZMYGR"
   },
   "source": [
    "there appear to be no breakage in my dataset and all the missing data appear to have been filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWtchZzRMYGR",
    "outputId": "0adc4311-4ccb-40c4-c0bc-7de9b9de60e2"
   },
   "outputs": [],
   "source": [
    "print('Number of pickup locations:', rides_jan.pickup_location_id.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTeeFMJDMYGR"
   },
   "source": [
    "### TIME SERIES TO FEATURES - January\n",
    "* I will use the time-series data i have created to create usefull features - one of them will be to show rides occurring in each hourly interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXtxh2WQMYGR"
   },
   "outputs": [],
   "source": [
    "#The idea behind this function is to create overlapping windows of data from a time series,\n",
    "# which can be especially useful for problems like forecasting. By having a window of n_features data points predict the next data point,\n",
    "# we're essentially framing a forecasting problem as a supervised learning problem, allowing traditional machine learning models to be applied on time series data.\n",
    "#we trying to take partition out data in 1 hours interval across 24 hours - We took the indicies\n",
    "def get_cutoff_indices(\n",
    "    data: pd.DataFrame,\n",
    "    n_features: int,\n",
    "    step_size: int\n",
    "    ) -> list:\n",
    "\n",
    "        stop_position = len(data) - 1 # Defines the final index position in the data. This ensures that the sub-sequences we create don't overshoot the actual data length\n",
    "\n",
    "        # Start the first sub-sequence at index position 0 - mid_idx nfeatures\n",
    "        subseq_first_idx = 0\n",
    "        subseq_mid_idx = n_features\n",
    "        subseq_last_idx = n_features + 1   #This index denotes the position of our target data point.\n",
    "        indices = []\n",
    "\n",
    "        while subseq_last_idx <= stop_position:\n",
    "            indices.append((subseq_first_idx, subseq_mid_idx, subseq_last_idx))    #Continues creating new sub-sequences as long as the sub-sequence's end doesn't exceed the length of the data. - then each tuple in this list provides starting, middle and ending boundary for each bundary for a subsequence\n",
    "\n",
    "            subseq_first_idx += step_size\n",
    "            subseq_mid_idx += step_size\n",
    "            subseq_last_idx += step_size        #These lines progress the boundaries forward in the dataset by step_size, moving to the next overlapping window.\n",
    "\n",
    "        return indices                           #The function ends by returning the populated list of index boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaRMqDE4MYGR"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfR8aJKaMYGR"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def transform_ts_data_into_features_and_target(\n",
    "    ts_data: pd.DataFrame,\n",
    "    input_seq_len: int,\n",
    "    step_size: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Slices and transposes data from time-series format into a (features, target)\n",
    "    format that we can use to train Supervised ML models\n",
    "    \"\"\"\n",
    "    assert set(ts_data.columns) == {'pickup_hour', 'number_of_rides', 'pickup_location_id'}\n",
    "\n",
    "    location_ids = ts_data['pickup_location_id'].unique()\n",
    "    features = pd.DataFrame()\n",
    "    targets = pd.DataFrame()\n",
    "\n",
    "    for location_id in tqdm(location_ids):\n",
    "\n",
    "        # keep only ts data for this `location_id`\n",
    "        ts_data_one_location = ts_data.loc[\n",
    "            ts_data.pickup_location_id == location_id,\n",
    "            ['pickup_hour', 'number_of_rides']\n",
    "        ]\n",
    "\n",
    "        # pre-compute cutoff indices to split dataframe rows\n",
    "        indices = get_cutoff_indices(\n",
    "            ts_data_one_location,\n",
    "            input_seq_len,\n",
    "            step_size\n",
    "        )\n",
    "\n",
    "        # slice and transpose data into numpy arrays for features and targets\n",
    "        n_examples = len(indices)\n",
    "        x = np.ndarray(shape=(n_examples, input_seq_len), dtype=np.float32)\n",
    "        y = np.ndarray(shape=(n_examples), dtype=np.float32)\n",
    "        pickup_hours = []\n",
    "        for i, idx in enumerate(indices):\n",
    "            x[i, :] = ts_data_one_location.iloc[idx[0]:idx[1]]['number_of_rides'].values\n",
    "            y[i] = ts_data_one_location.iloc[idx[1]:idx[2]]['number_of_rides'].values\n",
    "            pickup_hours.append(ts_data_one_location.iloc[idx[1]]['pickup_hour'])\n",
    "\n",
    "        # numpy -> pandas\n",
    "        features_one_location = pd.DataFrame(\n",
    "            x,\n",
    "            columns=[f'rides_previous_{i+1}_hour' for i in reversed(range(input_seq_len))]\n",
    "        )\n",
    "        features_one_location['pickup_hour'] = pickup_hours\n",
    "        features_one_location['pickup_location_id'] = location_id\n",
    "\n",
    "        # numpy -> pandas\n",
    "        targets_one_location = pd.DataFrame(y, columns=[f'target_rides_next_hour'])\n",
    "\n",
    "        # concatenate results\n",
    "        features = pd.concat([features, features_one_location])\n",
    "        targets = pd.concat([targets, targets_one_location])\n",
    "\n",
    "    features.reset_index(inplace=True, drop=True)\n",
    "    targets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return features, targets['target_rides_next_hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ytV_aQnMYGS",
    "outputId": "830559fa-aa1f-45ae-9e18-448b5dfa227f"
   },
   "outputs": [],
   "source": [
    "features, targets = transform_ts_data_into_features_and_target(\n",
    "    agg_rides_all_slots_jan,\n",
    "    input_seq_len=24*27*1, # one week of history\n",
    "    step_size=24,\n",
    ")\n",
    "\n",
    "print(f'{features.shape=}')\n",
    "print(f'{targets.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqwBAQfIMYGS",
    "outputId": "6100f82d-8daa-4912-f2cf-bbf0f8a52058"
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzkotuO3MYGS",
    "outputId": "5df3542f-aa34-4964-ae6e-99398bc700b3"
   },
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SemyVse3MYGS"
   },
   "outputs": [],
   "source": [
    "tabular_data_jan = features\n",
    "tabular_data_jan['target_rides_next_hour'] = targets\n",
    "\n",
    "tabular_data_jan.to_parquet('tabular_data_jan.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oxu7bMatMYGS",
    "outputId": "443138f9-3517-4c81-de09-01dcded299be"
   },
   "outputs": [],
   "source": [
    "tabular_data_jan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhC_51PJMYGT"
   },
   "source": [
    "### Preprocess for Feburary - (will do this for every month and combine during visuallization(EDA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bxj6KnpgMYGT"
   },
   "outputs": [],
   "source": [
    "rides_feb = pd.read_parquet('/Users/chetanhalai/Documents/code base for projects/4) uber/FINAL TAXI PROJECT 2023/data/2023 - FEB/yellow_tripdata_2023-02.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vT6vw7kSMYGT",
    "outputId": "858e6203-4c8f-48fc-e9e2-0146667393f2"
   },
   "outputs": [],
   "source": [
    "print(\"February month data shape:\", rides_feb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wet3dMWvMYGT",
    "outputId": "899d2881-cac0-4bdd-cdd1-35ca93cad68c"
   },
   "outputs": [],
   "source": [
    "rides_feb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-j7JkexYMYGT"
   },
   "outputs": [],
   "source": [
    "rides_feb = rides_feb [['tpep_pickup_datetime', 'PULocationID' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfesftsVMYGU"
   },
   "outputs": [],
   "source": [
    "rides_feb.rename(columns={'tpep_pickup_datetime': 'pickup_datetime',\n",
    "                      'PULocationID': 'pickup_location_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mta1k1nUMYGU",
    "outputId": "70ebdd28-a514-49d6-d807-806da6a697f9"
   },
   "outputs": [],
   "source": [
    "rides_feb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3T9fQCSMYGU",
    "outputId": "199b70c0-6a62-4f3a-c249-a98ac82d1d91"
   },
   "outputs": [],
   "source": [
    "#isolated the data that falls between 2023-02-01 and 2023-03-01'\n",
    "rides_feb = rides_feb[rides_feb.pickup_datetime >= '2023-02-01']\n",
    "rides_feb = rides_feb[rides_feb.pickup_datetime < '2023-03-01']\n",
    "rides_feb ['pickup_datetime'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "676uU2Z8MYGU",
    "outputId": "b394536a-b753-4c47-bd4a-5548820ffb18"
   },
   "outputs": [],
   "source": [
    "rides_feb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nfisfi74MYGU"
   },
   "outputs": [],
   "source": [
    "#Rounded data to the hour and will work with data in hourly intervals\n",
    "rides_feb['pickup_hour'] = rides_feb['pickup_datetime'].dt.floor('H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8M66-snmMYGV",
    "outputId": "fc9e2c4d-2aef-4938-dc5f-137680ba4474"
   },
   "outputs": [],
   "source": [
    "#going to group location ID and and pick up hour - (.size will give me the count of ride per hour - .reset_index goives me the data in ascending order)\n",
    "agg_rides_feb = rides_feb.groupby(['pickup_hour', 'pickup_location_id']).size().reset_index()\n",
    "agg_rides_feb.rename(columns={0: 'number_of_rides'}, inplace=True) #inplace=True is permenant\n",
    "agg_rides_feb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujnZXV2pMYGV",
    "outputId": "32addb5d-402a-4a42-ed05-018d52cb24ba"
   },
   "outputs": [],
   "source": [
    "agg_rides_feb.shape # i want make surei  have a complete time series - the data above appears to have eliminated rows that have no rides. What has occiures is 'pcikup hours' and 'location id' that had no ride were eliminiate from the data\n",
    "#there is no row - I want to add rows that had no rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dseqjcLOMYGV"
   },
   "outputs": [],
   "source": [
    "#this functions purpose is to fill in the data that was not present in the original data because there where no rides - time series data requires ther to be data in every time slot regardless of whether it existed or not\n",
    "from tqdm import tqdm\n",
    "\n",
    "def add_missing_slots(agg_rides: pd.DataFrame) -> pd.DataFrame: #take in the aggressed dataset that i presievely made and create an output a new DF which is going to be a complete timeseries\n",
    "\n",
    "    location_ids = agg_rides['pickup_location_id'].unique() # extract all the location ids and the full range of dates (bellow)...\n",
    "    full_range = pd.date_range(\n",
    "        agg_rides['pickup_hour'].min(), agg_rides['pickup_hour'].max(), freq='H') # ... from the first date till the last date and then building a series hour by hour\n",
    "    output = pd.DataFrame()\n",
    "    for location_id in tqdm(location_ids): #I did this operation per location ID .....\n",
    "\n",
    "        # keep only rides for this 'location_id'\n",
    "        agg_rides_i = agg_rides.loc[agg_rides.pickup_location_id == location_id, ['pickup_hour', 'number_of_rides']] #.... so i filter the data for onlyt that location ID and then we do this operatiom\n",
    "\n",
    "        # quick way to add missing dates with 0 in a Series\n",
    "        # taken from https://stackoverflow.com/a/19324591\n",
    "        agg_rides_i.set_index('pickup_hour', inplace=True) #this is the Reindex operation which basically takes in a df, (agg_rides_i), and then a full range of dates i want to have ...\n",
    "        agg_rides_i.index = pd.DatetimeIndex(agg_rides_i.index) #..... to ensure the out put df, has as many rows(and dates ) as the full range ......\n",
    "        agg_rides_i = agg_rides_i.reindex(full_range, fill_value=0) #... if thats not tne case it will fill it by 0)\n",
    "\n",
    "        # add back `location_id` columns\n",
    "        agg_rides_i['pickup_location_id'] = location_id #I then add back the location ID in this final dataframe\n",
    "\n",
    "        output = pd.concat([output, agg_rides_i]) # the i concatinate the results with the ones i got from the previous iterations and loop location id by location id\n",
    "\n",
    "    # move the purchase_day from the index to a dataframe column\n",
    "    output = output.reset_index().rename(columns={'index': 'pickup_hour'}) #then create a new column name, so this is why we need to add this rename column to inert thje index\n",
    "                                                                #which is now the datetime as a column and then rename it and its the pickup_hour\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pmzKforzMYGV",
    "outputId": "c3bbd6fe-a3a7-4efd-d4b9-301535de4d22"
   },
   "outputs": [],
   "source": [
    "print(agg_rides_feb.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ks7cg55EMYGW",
    "outputId": "d34b8688-fed0-4237-f964-80e0930fff05"
   },
   "outputs": [],
   "source": [
    "#calling the function and adding the missing slots\n",
    "agg_rides_all_slots_feb = add_missing_slots(agg_rides_feb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pbgjyhzgMYGW"
   },
   "outputs": [],
   "source": [
    "#visualise the data to detect bigs in my data processing - check if the graph has any breaks in it\n",
    "from typing import Optional, List\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_rides(\n",
    "    rides: pd.DataFrame,\n",
    "    locations: Optional[List[int]] = None\n",
    "    ):     #this function takes in the dataframe i made previously (rides) and the optionally takes in the locations, the ones that will be plotted\n",
    "    \"\"\"\"\n",
    "    Plot time-series data\n",
    "    \"\"\"\n",
    "    rides_to_plot = rides[rides.pickup_location_id.isin(locations)] if locations else rides #once i filtered the data frame I just call the plotly dot line function and it will plot multiple lines\n",
    "\n",
    "    fig = px.line(\n",
    "        rides_to_plot,\n",
    "        x=\"pickup_hour\",\n",
    "        y=\"number_of_rides\",\n",
    "        color='pickup_location_id',\n",
    "        template='none',\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xmqRgjeMYGW",
    "outputId": "97558404-a768-4bb6-b240-e7c5ea9336b0"
   },
   "outputs": [],
   "source": [
    "plot_rides(agg_rides_all_slots_feb, locations=[261])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7R6ZwGbnMYGW",
    "outputId": "2b8f86a0-2b39-4a55-e6c0-e3db43c381b9"
   },
   "outputs": [],
   "source": [
    "print('Number of pickup locations:', rides_feb.pickup_location_id.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etJUZDAeMYGW"
   },
   "source": [
    "### TIME SERIES TO FEATURES - February\n",
    "* I will use the time-series data i have created to create usefull features - one of them will be to show rides occurring in each hourly interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vV7a_hyjMYGW"
   },
   "outputs": [],
   "source": [
    "#The idea behind this function is to create overlapping windows of data from a time series,\n",
    "# which can be especially useful for problems like forecasting. By having a window of n_features data points predict the next data point,\n",
    "# we're essentially framing a forecasting problem as a supervised learning problem, allowing traditional machine learning models to be applied on time series data.\n",
    "#we trying to take partition out data in 1 hours interval across 24 hours - We took the indicies\n",
    "def get_cutoff_indices(\n",
    "    data: pd.DataFrame,\n",
    "    n_features: int,\n",
    "    step_size: int\n",
    "    ) -> list:\n",
    "\n",
    "        stop_position = len(data) - 1 # Defines the final index position in the data. This ensures that the sub-sequences we create don't overshoot the actual data length\n",
    "\n",
    "        # Start the first sub-sequence at index position 0 - mid_idx nfeatures\n",
    "        subseq_first_idx = 0\n",
    "        subseq_mid_idx = n_features\n",
    "        subseq_last_idx = n_features + 1   #This index denotes the position of our target data point.\n",
    "        indices = []\n",
    "\n",
    "        while subseq_last_idx <= stop_position:\n",
    "            indices.append((subseq_first_idx, subseq_mid_idx, subseq_last_idx))    #Continues creating new sub-sequences as long as the sub-sequence's end doesn't exceed the length of the data. - then each tuple in this list provides starting, middle and ending boundary for each bundary for a subsequence\n",
    "\n",
    "            subseq_first_idx += step_size\n",
    "            subseq_mid_idx += step_size\n",
    "            subseq_last_idx += step_size        #These lines progress the boundaries forward in the dataset by step_size, moving to the next overlapping window.\n",
    "\n",
    "        return indices                           #The function ends by returning the populated list of index boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab9J6k0OMYGX"
   },
   "outputs": [],
   "source": [
    "features, targets = transform_ts_data_into_features_and_target(\n",
    "    agg_rides_all_slots_feb,\n",
    "    input_seq_len=24*27*1, # one week of history\n",
    "    step_size=24,\n",
    ")\n",
    "\n",
    "print(f'{features.shape=}')\n",
    "print(f'{targets.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGixo-OsMYGX",
    "outputId": "7840b9f0-6034-4b06-ca3e-0e88dbc1e027"
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JjPcFVLNMYGX",
    "outputId": "324ebd44-f8c3-4e6c-ef02-05d4f30e8a7e"
   },
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Qk2O_bLMYGX"
   },
   "outputs": [],
   "source": [
    "tabular_data_feb = features\n",
    "tabular_data_feb['target_rides_next_hour'] = targets\n",
    "\n",
    "tabular_data_feb.to_parquet('tabular_data_feb.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owR9UqRZMYGX",
    "outputId": "c8b67b4e-231e-4b26-82f4-44e1fdcfc883"
   },
   "outputs": [],
   "source": [
    "tabular_data_feb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9EiARO8MYGY"
   },
   "source": [
    "### Preprocess for March - (will do this for every month and combine during visuallization(EDA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXB8eqfhMYGY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtwp2GymMYGY"
   },
   "outputs": [],
   "source": [
    "rides_mar = pd.read_parquet('/Users/chetanhalai/Documents/code base for projects/4) uber/FINAL TAXI PROJECT 2023/data/2023 - MAR/yellow_tripdata_2023-03.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLQ0o_MpMYGY",
    "outputId": "251f4ba9-de95-48ba-9924-fca900b20055"
   },
   "outputs": [],
   "source": [
    "print(\"March month data shape:\", rides_mar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ean15cLlMYGY",
    "outputId": "ea8b258e-a562-4780-9001-252ffb010447"
   },
   "outputs": [],
   "source": [
    "rides_mar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tx2_UMEUMYGY"
   },
   "outputs": [],
   "source": [
    "rides_mar = rides_mar [['tpep_pickup_datetime', 'PULocationID' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDbZzBU3MYGZ"
   },
   "outputs": [],
   "source": [
    "rides_mar.rename(columns={'tpep_pickup_datetime': 'pickup_datetime',\n",
    "                      'PULocationID': 'pickup_location_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXPXgt3OMYGZ"
   },
   "outputs": [],
   "source": [
    "rides_mar.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-KLHES5MYGZ"
   },
   "outputs": [],
   "source": [
    "#isolated the data that falls between 2023-03-01 and 2023-04-01'\n",
    "rides_mar = rides_mar[rides_mar.pickup_datetime >= '2023-03-01']\n",
    "rides_mar = rides_mar[rides_mar.pickup_datetime < '2023-04-01']\n",
    "rides_mar ['pickup_datetime'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AdyADhu6MYGZ"
   },
   "outputs": [],
   "source": [
    "#Rounded data to the hour and will work with data in hourly intervals\n",
    "rides_mar['pickup_hour'] = rides_mar['pickup_datetime'].dt.floor('H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6W9ZffqMYGZ"
   },
   "outputs": [],
   "source": [
    "#going to group location ID and and pick up hour - (.size will give me the count of ride per hour - .reset_index goives me the data in ascending order)\n",
    "agg_rides_mar = rides_mar.groupby(['pickup_hour', 'pickup_location_id']).size().reset_index()\n",
    "agg_rides_mar.rename(columns={0: 'number_of_rides'}, inplace=True) #inplace=True is permenant\n",
    "agg_rides_mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7GzF4-KMYGZ"
   },
   "outputs": [],
   "source": [
    "agg_rides_mar.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0xAOpmIMYGa"
   },
   "outputs": [],
   "source": [
    "#this functions purpose is to fill in the data that was not present in the original data because there where no rides - time series data requires ther to be data in every time slot regardless of whether it existed or not\n",
    "from tqdm import tqdm\n",
    "\n",
    "def add_missing_slots(agg_rides: pd.DataFrame) -> pd.DataFrame: #take in the aggressed dataset that i presievely made and create an output a new DF which is going to be a complete timeseries\n",
    "\n",
    "    location_ids = agg_rides['pickup_location_id'].unique() # extract all the location ids and the full range of dates (bellow)...\n",
    "    full_range = pd.date_range(\n",
    "        agg_rides['pickup_hour'].min(), agg_rides['pickup_hour'].max(), freq='H') # ... from the first date till the last date and then building a series hour by hour\n",
    "    output = pd.DataFrame()\n",
    "    for location_id in tqdm(location_ids): #I did this operation per location ID .....\n",
    "\n",
    "        # keep only rides for this 'location_id'\n",
    "        agg_rides_i = agg_rides.loc[agg_rides.pickup_location_id == location_id, ['pickup_hour', 'number_of_rides']] #.... so i filter the data for onlyt that location ID and then we do this operatiom\n",
    "\n",
    "        # quick way to add missing dates with 0 in a Series\n",
    "        # taken from https://stackoverflow.com/a/19324591\n",
    "        agg_rides_i.set_index('pickup_hour', inplace=True) #this is the Reindex operation which basically takes in a df, (agg_rides_i), and then a full range of dates i want to have ...\n",
    "        agg_rides_i.index = pd.DatetimeIndex(agg_rides_i.index) #..... to ensure the out put df, has as many rows(and dates ) as the full range ......\n",
    "        agg_rides_i = agg_rides_i.reindex(full_range, fill_value=0) #... if thats not tne case it will fill it by 0)\n",
    "\n",
    "        # add back `location_id` columns\n",
    "        agg_rides_i['pickup_location_id'] = location_id #I then add back the location ID in this final dataframe\n",
    "\n",
    "        output = pd.concat([output, agg_rides_i]) # the i concatinate the results with the ones i got from the previous iterations and loop location id by location id\n",
    "\n",
    "    # move the purchase_day from the index to a dataframe column\n",
    "    output = output.reset_index().rename(columns={'index': 'pickup_hour'}) #then create a new column name, so this is why we need to add this rename column to inert thje index\n",
    "                                                                #which is now the datetime as a column and then rename it and its the pickup_hour\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNDFY3imMYGa"
   },
   "outputs": [],
   "source": [
    "print(agg_rides_mar.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1iuR1i5MYGa"
   },
   "outputs": [],
   "source": [
    "#calling the function and adding the missing slots\n",
    "agg_rides_all_slots_mar = add_missing_slots(agg_rides_mar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-r9IoO08MYGa"
   },
   "outputs": [],
   "source": [
    "#visualise the data to detect bigs in my data processing - check if the graph has any breaks in it\n",
    "from typing import Optional, List\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_rides(\n",
    "    rides: pd.DataFrame,\n",
    "    locations: Optional[List[int]] = None\n",
    "    ):     #this function takes in the dataframe i made previously (rides) and the optionally takes in the locations, the ones that will be plotted\n",
    "    \"\"\"\"\n",
    "    Plot time-series data\n",
    "    \"\"\"\n",
    "    rides_to_plot = rides[rides.pickup_location_id.isin(locations)] if locations else rides #once i filtered the data frame I just call the plotly dot line function and it will plot multiple lines\n",
    "\n",
    "    fig = px.line(\n",
    "        rides_to_plot,\n",
    "        x=\"pickup_hour\",\n",
    "        y=\"number_of_rides\",\n",
    "        color='pickup_location_id',\n",
    "        template='none',\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tHK2TbeMYGa"
   },
   "outputs": [],
   "source": [
    "plot_rides(agg_rides_all_slots_mar, locations=[261])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQN06Z7GMYGa"
   },
   "outputs": [],
   "source": [
    "print('Number of pickup locations:', rides_mar.pickup_location_id.nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bohA5oKMMYGa"
   },
   "source": [
    "### Time series - turning time series data to features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVPJKod4MYGa"
   },
   "outputs": [],
   "source": [
    "#The idea behind this function is to create overlapping windows of data from a time series,\n",
    "# which can be especially useful for problems like forecasting. By having a window of n_features data points predict the next data point,\n",
    "# we're essentially framing a forecasting problem as a supervised learning problem, allowing traditional machine learning models to be applied on time series data.\n",
    "#we trying to take partition out data in 1 hours interval across 24 hours - We took the indicies\n",
    "def get_cutoff_indices(\n",
    "    data: pd.DataFrame,\n",
    "    n_features: int,\n",
    "    step_size: int\n",
    "    ) -> list:\n",
    "\n",
    "        stop_position = len(data) - 1 # Defines the final index position in the data. This ensures that the sub-sequences we create don't overshoot the actual data length\n",
    "\n",
    "        # Start the first sub-sequence at index position 0 - mid_idx nfeatures\n",
    "        subseq_first_idx = 0\n",
    "        subseq_mid_idx = n_features\n",
    "        subseq_last_idx = n_features + 1   #This index denotes the position of our target data point.\n",
    "        indices = []\n",
    "\n",
    "        while subseq_last_idx <= stop_position:\n",
    "            indices.append((subseq_first_idx, subseq_mid_idx, subseq_last_idx))    #Continues creating new sub-sequences as long as the sub-sequence's end doesn't exceed the length of the data. - then each tuple in this list provides starting, middle and ending boundary for each bundary for a subsequence\n",
    "\n",
    "            subseq_first_idx += step_size\n",
    "            subseq_mid_idx += step_size\n",
    "            subseq_last_idx += step_size        #These lines progress the boundaries forward in the dataset by step_size, moving to the next overlapping window.\n",
    "\n",
    "        return indices                           #The function ends by returning the populated list of index boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "231Hl2iVMYGc"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def transform_ts_data_into_features_and_target(\n",
    "    ts_data: pd.DataFrame,\n",
    "    input_seq_len: int,\n",
    "    step_size: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Slices and transposes data from time-series format into a (features, target)\n",
    "    format that we can use to train Supervised ML models\n",
    "    \"\"\"\n",
    "    assert set(ts_data.columns) == {'pickup_hour', 'number_of_rides', 'pickup_location_id'}\n",
    "\n",
    "    location_ids = ts_data['pickup_location_id'].unique()\n",
    "    features = pd.DataFrame()\n",
    "    targets = pd.DataFrame()\n",
    "\n",
    "    for location_id in tqdm(location_ids):\n",
    "\n",
    "        # keep only ts data for this `location_id`\n",
    "        ts_data_one_location = ts_data.loc[\n",
    "            ts_data.pickup_location_id == location_id,\n",
    "            ['pickup_hour', 'number_of_rides']\n",
    "        ]\n",
    "\n",
    "        # pre-compute cutoff indices to split dataframe rows\n",
    "        indices = get_cutoff_indices(\n",
    "            ts_data_one_location,\n",
    "            input_seq_len,\n",
    "            step_size\n",
    "        )\n",
    "\n",
    "        # slice and transpose data into numpy arrays for features and targets\n",
    "        n_examples = len(indices)\n",
    "        x = np.ndarray(shape=(n_examples, input_seq_len), dtype=np.float32)\n",
    "        y = np.ndarray(shape=(n_examples), dtype=np.float32)\n",
    "        pickup_hours = []\n",
    "        for i, idx in enumerate(indices):\n",
    "            x[i, :] = ts_data_one_location.iloc[idx[0]:idx[1]]['number_of_rides'].values\n",
    "            y[i] = ts_data_one_location.iloc[idx[1]:idx[2]]['number_of_rides'].values\n",
    "            pickup_hours.append(ts_data_one_location.iloc[idx[1]]['pickup_hour'])\n",
    "\n",
    "        # numpy -> pandas\n",
    "        features_one_location = pd.DataFrame(\n",
    "            x,\n",
    "            columns=[f'rides_previous_{i+1}_hour' for i in reversed(range(input_seq_len))]\n",
    "        )\n",
    "        features_one_location['pickup_hour'] = pickup_hours\n",
    "        features_one_location['pickup_location_id'] = location_id\n",
    "\n",
    "        # numpy -> pandas\n",
    "        targets_one_location = pd.DataFrame(y, columns=[f'target_rides_next_hour'])\n",
    "\n",
    "        # concatenate results\n",
    "        features = pd.concat([features, features_one_location])\n",
    "        targets = pd.concat([targets, targets_one_location])\n",
    "\n",
    "    features.reset_index(inplace=True, drop=True)\n",
    "    targets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return features, targets['target_rides_next_hour']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXUIHC1FMYGc"
   },
   "outputs": [],
   "source": [
    "features, targets = transform_ts_data_into_features_and_target(\n",
    "    agg_rides_all_slots_mar,\n",
    "    input_seq_len=24*27*1, # one week of history\n",
    "    step_size=24,\n",
    ")\n",
    "\n",
    "print(f'{features.shape=}')\n",
    "print(f'{targets.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnP-FYPUMYGc"
   },
   "outputs": [],
   "source": [
    "features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XyOxPSbMYGc"
   },
   "outputs": [],
   "source": [
    "targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mn97hHFfMYGc"
   },
   "outputs": [],
   "source": [
    "tabular_data_mar = features\n",
    "tabular_data_mar['target_rides_next_hour'] = targets\n",
    "\n",
    "tabular_data_mar.to_parquet('tabular_data_mar.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dgu6MLeEMYGc"
   },
   "outputs": [],
   "source": [
    "tabular_data_mar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess for April- (will do this for every month and combine during visuallization(EDA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zt1iHOhxMYGd"
   },
   "outputs": [],
   "source": [
    "rides_apr = pd.read_parquet('/Users/chetanhalai/Documents/code base for projects/4) uber/FINAL TAXI PROJECT 2023/data/2023 - APR/yellow_tripdata_2023-04.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mU-CVEZAMYGd"
   },
   "outputs": [],
   "source": [
    "print(\"April month data shape:\", rides_apr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kw8cDPt2MYGd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kP79EoT9MYGd"
   },
   "outputs": [],
   "source": [
    "rides_apr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qcnzyWc-MYGe"
   },
   "outputs": [],
   "source": [
    "#isolated the data that falls between 2023-01-01 and 2023-02-01'\n",
    "# rides_feb = rides_feb[rides_feb.pickup_datetime >= '2023-02-01'] rides_feb = rides_feb[rides_feb.pickup_datetime < '2023-03-01'] rides_feb ['pickup_datetime'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hz5qeF_3MYGe"
   },
   "outputs": [],
   "source": [
    "rides_apr = rides_apr [['tpep_pickup_datetime', 'PULocationID' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1G-AMlYMYGe"
   },
   "outputs": [],
   "source": [
    "rides_apr.rename(columns={'tpep_pickup_datetime': 'pickup_datetime',\n",
    "                      'PULocationID': 'pickup_location_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ladrhwdzMYGe"
   },
   "outputs": [],
   "source": [
    "rides_apr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnoSZVUnMYGe"
   },
   "outputs": [],
   "source": [
    "#isolated the data that falls between 2023-04-01 and 2023-05-01'\n",
    "rides_apr = rides_apr[rides_apr.pickup_datetime >= '2023-04-01']\n",
    "rides_apr = rides_apr[rides_apr.pickup_datetime < '2023-05-01']\n",
    "rides_apr ['pickup_datetime'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aqoz6_aPMYGe"
   },
   "outputs": [],
   "source": [
    "#Rounded data to the hour and will work with data in hourly intervals\n",
    "rides_apr['pickup_hour'] = rides_apr['pickup_datetime'].dt.floor('H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xaPXvvXMMYGe"
   },
   "outputs": [],
   "source": [
    "\n",
    "#going to group location ID and and pick up hour - (.size will give me the count of ride per hour - .reset_index goives me the data in ascending order)\n",
    "agg_rides_apr = rides_apr.groupby(['pickup_hour', 'pickup_location_id']).size().reset_index()\n",
    "agg_rides_apr.rename(columns={0: 'number_of_rides'}, inplace=True) #inplace=True is permenant\n",
    "agg_rides_apr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HRscozMEMYGe"
   },
   "outputs": [],
   "source": [
    "agg_rides_apr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpSGrYnLMYGf"
   },
   "outputs": [],
   "source": [
    "#this functions purpose is to fill in the data that was not present in the original data because there where no rides - time series data requires ther to be data in every time slot regardless of whether it existed or not\n",
    "from tqdm import tqdm\n",
    "\n",
    "def add_missing_slots(agg_rides: pd.DataFrame) -> pd.DataFrame: #take in the aggressed dataset that i presievely made and create an output a new DF which is going to be a complete timeseries\n",
    "\n",
    "    location_ids = agg_rides['pickup_location_id'].unique() # extract all the location ids and the full range of dates (bellow)...\n",
    "    full_range = pd.date_range(\n",
    "        agg_rides['pickup_hour'].min(), agg_rides['pickup_hour'].max(), freq='H') # ... from the first date till the last date and then building a series hour by hour\n",
    "    output = pd.DataFrame()\n",
    "    for location_id in tqdm(location_ids): #I did this operation per location ID .....\n",
    "\n",
    "        # keep only rides for this 'location_id'\n",
    "        agg_rides_i = agg_rides.loc[agg_rides.pickup_location_id == location_id, ['pickup_hour', 'number_of_rides']] #.... so i filter the data for onlyt that location ID and then we do this operatiom\n",
    "\n",
    "        # quick way to add missing dates with 0 in a Series\n",
    "        # taken from https://stackoverflow.com/a/19324591\n",
    "        agg_rides_i.set_index('pickup_hour', inplace=True) #this is the Reindex operation which basically takes in a df, (agg_rides_i), and then a full range of dates i want to have ...\n",
    "        agg_rides_i.index = pd.DatetimeIndex(agg_rides_i.index) #..... to ensure the out put df, has as many rows(and dates ) as the full range ......\n",
    "        agg_rides_i = agg_rides_i.reindex(full_range, fill_value=0) #... if thats not tne case it will fill it by 0)\n",
    "\n",
    "        # add back `location_id` columns\n",
    "        agg_rides_i['pickup_location_id'] = location_id #I then add back the location ID in this final dataframe\n",
    "\n",
    "        output = pd.concat([output, agg_rides_i]) # the i concatinate the results with the ones i got from the previous iterations and loop location id by location id\n",
    "\n",
    "    # move the purchase_day from the index to a dataframe column\n",
    "    output = output.reset_index().rename(columns={'index': 'pickup_hour'}) #then create a new column name, so this is why we need to add this rename column to inert thje index\n",
    "                                                                #which is now the datetime as a column and then rename it and its the pickup_hour\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6W7DDfC4MYGf"
   },
   "outputs": [],
   "source": [
    "print(agg_rides_apr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXy5ZrxVMYGf"
   },
   "outputs": [],
   "source": [
    "#calling the function and adding the missing slots\n",
    "agg_rides_all_slots_apr = add_missing_slots(agg_rides_apr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cxa0yWohMYGf"
   },
   "outputs": [],
   "source": [
    "#visualise the data to detect bigs in my data processing - check if the graph has any breaks in it\n",
    "from typing import Optional, List\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_rides(\n",
    "    rides: pd.DataFrame,\n",
    "    locations: Optional[List[int]] = None\n",
    "    ):     #this function takes in the dataframe i made previously (rides) and the optionally takes in the locations, the ones that will be plotted\n",
    "    \"\"\"\"\n",
    "    Plot time-series data\n",
    "    \"\"\"\n",
    "    rides_to_plot = rides[rides.pickup_location_id.isin(locations)] if locations else rides #once i filtered the data frame I just call the plotly dot line function and it will plot multiple lines\n",
    "\n",
    "    fig = px.line(\n",
    "        rides_to_plot,\n",
    "        x=\"pickup_hour\",\n",
    "        y=\"number_of_rides\",\n",
    "        color='pickup_location_id',\n",
    "        template='none',\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2AFMSagxMYGg"
   },
   "outputs": [],
   "source": [
    "plot_rides(agg_rides_all_slots_apr, locations=[261])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBQ1fuR2MYGg"
   },
   "outputs": [],
   "source": [
    "print('Number of pickup locations:', rides_apr.pickup_location_id.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "go6rm40JMYGg"
   },
   "source": [
    "### time series data to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UY-NhMLaMYGg"
   },
   "outputs": [],
   "source": [
    "#The idea behind this function is to create overlapping windows of data from a time series,\n",
    "# which can be especially useful for problems like forecasting. By having a window of n_features data points predict the next data point,\n",
    "# we're essentially framing a forecasting problem as a supervised learning problem, allowing traditional machine learning models to be applied on time series data.\n",
    "#we trying to take partition out data in 1 hours interval across 24 hours - We took the indicies\n",
    "def get_cutoff_indices(\n",
    "    data: pd.DataFrame,\n",
    "    n_features: int,\n",
    "    step_size: int\n",
    "    ) -> list:\n",
    "\n",
    "        stop_position = len(data) - 1 # Defines the final index position in the data. This ensures that the sub-sequences we create don't overshoot the actual data length\n",
    "\n",
    "        # Start the first sub-sequence at index position 0 - mid_idx nfeatures\n",
    "        subseq_first_idx = 0\n",
    "        subseq_mid_idx = n_features\n",
    "        subseq_last_idx = n_features + 1   #This index denotes the position of our target data point.\n",
    "        indices = []\n",
    "\n",
    "        while subseq_last_idx <= stop_position:\n",
    "            indices.append((subseq_first_idx, subseq_mid_idx, subseq_last_idx))    #Continues creating new sub-sequences as long as the sub-sequence's end doesn't exceed the length of the data. - then each tuple in this list provides starting, middle and ending boundary for each bundary for a subsequence\n",
    "\n",
    "            subseq_first_idx += step_size\n",
    "            subseq_mid_idx += step_size\n",
    "            subseq_last_idx += step_size        #These lines progress the boundaries forward in the dataset by step_size, moving to the next overlapping window.\n",
    "\n",
    "        return indices                           #The function ends by returning the populated list of index boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRvAa1JqMYGg"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def transform_ts_data_into_features_and_target(\n",
    "    ts_data: pd.DataFrame,\n",
    "    input_seq_len: int,\n",
    "    step_size: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Slices and transposes data from time-series format into a (features, target)\n",
    "    format that we can use to train Supervised ML models\n",
    "    \"\"\"\n",
    "    assert set(ts_data.columns) == {'pickup_hour', 'number_of_rides', 'pickup_location_id'}\n",
    "\n",
    "    location_ids = ts_data['pickup_location_id'].unique()\n",
    "    features = pd.DataFrame()\n",
    "    targets = pd.DataFrame()\n",
    "\n",
    "    for location_id in tqdm(location_ids):\n",
    "\n",
    "        # keep only ts data for this `location_id`\n",
    "        ts_data_one_location = ts_data.loc[\n",
    "            ts_data.pickup_location_id == location_id,\n",
    "            ['pickup_hour', 'number_of_rides']\n",
    "        ]\n",
    "\n",
    "        # pre-compute cutoff indices to split dataframe rows\n",
    "        indices = get_cutoff_indices(\n",
    "            ts_data_one_location,\n",
    "            input_seq_len,\n",
    "            step_size\n",
    "        )\n",
    "\n",
    "        # slice and transpose data into numpy arrays for features and targets\n",
    "        n_examples = len(indices)\n",
    "        x = np.ndarray(shape=(n_examples, input_seq_len), dtype=np.float32)\n",
    "        y = np.ndarray(shape=(n_examples), dtype=np.float32)\n",
    "        pickup_hours = []\n",
    "        for i, idx in enumerate(indices):\n",
    "            x[i, :] = ts_data_one_location.iloc[idx[0]:idx[1]]['number_of_rides'].values\n",
    "            y[i] = ts_data_one_location.iloc[idx[1]:idx[2]]['number_of_rides'].values\n",
    "            pickup_hours.append(ts_data_one_location.iloc[idx[1]]['pickup_hour'])\n",
    "\n",
    "        # numpy -> pandas\n",
    "        features_one_location = pd.DataFrame(\n",
    "            x,\n",
    "            columns=[f'rides_previous_{i+1}_hour' for i in reversed(range(input_seq_len))]\n",
    "        )\n",
    "        features_one_location['pickup_hour'] = pickup_hours\n",
    "        features_one_location['pickup_location_id'] = location_id\n",
    "\n",
    "        # numpy -> pandas\n",
    "        targets_one_location = pd.DataFrame(y, columns=[f'target_rides_next_hour'])\n",
    "\n",
    "        # concatenate results\n",
    "        features = pd.concat([features, features_one_location])\n",
    "        targets = pd.concat([targets, targets_one_location])\n",
    "\n",
    "    features.reset_index(inplace=True, drop=True)\n",
    "    targets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return features, targets['target_rides_next_hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LqfdxuDqMYGg"
   },
   "outputs": [],
   "source": [
    "features, targets = transform_ts_data_into_features_and_target(\n",
    "    agg_rides_all_slots_apr,\n",
    "    input_seq_len=24*27*1, # one week of history\n",
    "    step_size=24,\n",
    ")\n",
    "\n",
    "print(f'{features.shape=}')\n",
    "print(f'{targets.shape=}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UETp3wrtMYGg"
   },
   "outputs": [],
   "source": [
    "features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pvi23e3BMYGi"
   },
   "outputs": [],
   "source": [
    "targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sb4SsP7rMYGi"
   },
   "outputs": [],
   "source": [
    "tabular_data_apr = features\n",
    "tabular_data_apr['target_rides_next_hour'] = targets\n",
    "\n",
    "tabular_data_apr.to_parquet('tabular_data_apr.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSwTDG4EMYGi"
   },
   "outputs": [],
   "source": [
    "tabular_data_apr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7o1Wq-yHMYGi"
   },
   "source": [
    "### Preprocess for May - (will do this for every month and combine during visuallization(EDA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jcrRNqFjMYGi"
   },
   "outputs": [],
   "source": [
    "rides_may = pd.read_parquet('/Users/chetanhalai/Documents/code base for projects/4) uber/FINAL TAXI PROJECT 2023/data/2023 - MAY/yellow_tripdata_2023-05.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tkFODGUMYGi"
   },
   "outputs": [],
   "source": [
    "print(\"April month data shape:\", rides_may.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmR4tRcPMYGi"
   },
   "outputs": [],
   "source": [
    "rides_may.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T95XIZQMMYGi"
   },
   "outputs": [],
   "source": [
    "rides_may = rides_may [['tpep_pickup_datetime', 'PULocationID' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WaGneZzGMYGi"
   },
   "outputs": [],
   "source": [
    "rides_may.rename(columns={'tpep_pickup_datetime': 'pickup_datetime',\n",
    "                      'PULocationID': 'pickup_location_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2r6NjCGpMYGi"
   },
   "outputs": [],
   "source": [
    "rides_may.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJmOfhUsMYGj"
   },
   "outputs": [],
   "source": [
    "#isolated the data that falls between 2023-01-01 and 2023-02-01'\n",
    "rides_may = rides_may[rides_may.pickup_datetime >= '2023-05-01']\n",
    "rides_may = rides_may[rides_may.pickup_datetime < '2023-06-01']\n",
    "rides_may ['pickup_datetime'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUOofHiOMYGj"
   },
   "outputs": [],
   "source": [
    "#Rounded data to the hour and will work with data in hourly intervals\n",
    "rides_may['pickup_hour'] = rides_may['pickup_datetime'].dt.floor('H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTFmfHxMMYGj"
   },
   "outputs": [],
   "source": [
    "#going to group location ID and and pick up hour - (.size will give me the count of ride per hour - .reset_index goives me the data in ascending order)\n",
    "agg_rides_may = rides_may.groupby(['pickup_hour', 'pickup_location_id']).size().reset_index()\n",
    "agg_rides_may.rename(columns={0: 'number_of_rides'}, inplace=True) #inplace=True is permenant\n",
    "agg_rides_may\n",
    "\n",
    "\n",
    "agg_rides_may.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "heDLL_Q4MYGj"
   },
   "outputs": [],
   "source": [
    "#this functions purpose is to fill in the data that was not present in the original data because there where no rides - time series data requires ther to be data in every time slot regardless of whether it existed or not\n",
    "from tqdm import tqdm\n",
    "\n",
    "def add_missing_slots(agg_rides: pd.DataFrame) -> pd.DataFrame: #take in the aggressed dataset that i presievely made and create an output a new DF which is going to be a complete timeseries\n",
    "\n",
    "    location_ids = agg_rides['pickup_location_id'].unique() # extract all the location ids and the full range of dates (bellow)...\n",
    "    full_range = pd.date_range(\n",
    "        agg_rides['pickup_hour'].min(), agg_rides['pickup_hour'].max(), freq='H') # ... from the first date till the last date and then building a series hour by hour\n",
    "    output = pd.DataFrame()\n",
    "    for location_id in tqdm(location_ids): #I did this operation per location ID .....\n",
    "\n",
    "        # keep only rides for this 'location_id'\n",
    "        agg_rides_i = agg_rides.loc[agg_rides.pickup_location_id == location_id, ['pickup_hour', 'number_of_rides']] #.... so i filter the data for onlyt that location ID and then we do this operatiom\n",
    "\n",
    "        # quick way to add missing dates with 0 in a Series\n",
    "        # taken from https://stackoverflow.com/a/19324591\n",
    "        agg_rides_i.set_index('pickup_hour', inplace=True) #this is the Reindex operation which basically takes in a df, (agg_rides_i), and then a full range of dates i want to have ...\n",
    "        agg_rides_i.index = pd.DatetimeIndex(agg_rides_i.index) #..... to ensure the out put df, has as many rows(and dates ) as the full range ......\n",
    "        agg_rides_i = agg_rides_i.reindex(full_range, fill_value=0) #... if thats not tne case it will fill it by 0)\n",
    "\n",
    "        # add back `location_id` columns\n",
    "        agg_rides_i['pickup_location_id'] = location_id #I then add back the location ID in this final dataframe\n",
    "\n",
    "        output = pd.concat([output, agg_rides_i]) # the i concatinate the results with the ones i got from the previous iterations and loop location id by location id\n",
    "\n",
    "    # move the purchase_day from the index to a dataframe column\n",
    "    output = output.reset_index().rename(columns={'index': 'pickup_hour'}) #then create a new column name, so this is why we need to add this rename column to inert thje index\n",
    "                                                                #which is now the datetime as a column and then rename it and its the pickup_hour\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8puXLuiAMYGk"
   },
   "outputs": [],
   "source": [
    "print(agg_rides_may.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fQjJOpyMYGk"
   },
   "outputs": [],
   "source": [
    "\n",
    "#calling the function and adding the missing slots\n",
    "agg_rides_all_slots_may = add_missing_slots(agg_rides_may)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEZfPzt5MYGk"
   },
   "outputs": [],
   "source": [
    "#visualise the data to detect bigs in my data processing - check if the graph has any breaks in it\n",
    "from typing import Optional, List\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_rides(\n",
    "    rides: pd.DataFrame,\n",
    "    locations: Optional[List[int]] = None\n",
    "    ):     #this function takes in the dataframe i made previously (rides) and the optionally takes in the locations, the ones that will be plotted\n",
    "    \"\"\"\"\n",
    "    Plot time-series data\n",
    "    \"\"\"\n",
    "    rides_to_plot = rides[rides.pickup_location_id.isin(locations)] if locations else rides #once i filtered the data frame I just call the plotly dot line function and it will plot multiple lines\n",
    "\n",
    "    fig = px.line(\n",
    "        rides_to_plot,\n",
    "        x=\"pickup_hour\",\n",
    "        y=\"number_of_rides\",\n",
    "        color='pickup_location_id',\n",
    "        template='none',\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Sfmdg12MYGk"
   },
   "outputs": [],
   "source": [
    "plot_rides(agg_rides_all_slots_may, locations=[261])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RLJztW-MYGk"
   },
   "outputs": [],
   "source": [
    "print('Number of pickup locations:', rides_may.pickup_location_id.nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twyfsqWMMYGk"
   },
   "source": [
    "TIME SERIES - TURNING TIME SERIES DATA INTO FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OBsJZxbSMYGk"
   },
   "outputs": [],
   "source": [
    "#The idea behind this function is to create overlapping windows of data from a time series,\n",
    "# which can be especially useful for problems like forecasting. By having a window of n_features data points predict the next data point,\n",
    "# we're essentially framing a forecasting problem as a supervised learning problem, allowing traditional machine learning models to be applied on time series data.\n",
    "#we trying to take partition out data in 1 hours interval across 24 hours - We took the indicies\n",
    "def get_cutoff_indices(\n",
    "    data: pd.DataFrame,\n",
    "    n_features: int,\n",
    "    step_size: int\n",
    "    ) -> list:\n",
    "\n",
    "        stop_position = len(data) - 1 # Defines the final index position in the data. This ensures that the sub-sequences we create don't overshoot the actual data length\n",
    "\n",
    "        # Start the first sub-sequence at index position 0 - mid_idx nfeatures\n",
    "        subseq_first_idx = 0\n",
    "        subseq_mid_idx = n_features\n",
    "        subseq_last_idx = n_features + 1   #This index denotes the position of our target data point.\n",
    "        indices = []\n",
    "\n",
    "        while subseq_last_idx <= stop_position:\n",
    "            indices.append((subseq_first_idx, subseq_mid_idx, subseq_last_idx))    #Continues creating new sub-sequences as long as the sub-sequence's end doesn't exceed the length of the data. - then each tuple in this list provides starting, middle and ending boundary for each bundary for a subsequence\n",
    "\n",
    "            subseq_first_idx += step_size\n",
    "            subseq_mid_idx += step_size\n",
    "            subseq_last_idx += step_size        #These lines progress the boundaries forward in the dataset by step_size, moving to the next overlapping window.\n",
    "\n",
    "        return indices                           #The function ends by returning the populated list of index boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZZRWN2ZMYGl"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def transform_ts_data_into_features_and_target(\n",
    "    ts_data: pd.DataFrame,\n",
    "    input_seq_len: int,\n",
    "    step_size: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Slices and transposes data from time-series format into a (features, target)\n",
    "    format that we can use to train Supervised ML models\n",
    "    \"\"\"\n",
    "    assert set(ts_data.columns) == {'pickup_hour', 'number_of_rides', 'pickup_location_id'}\n",
    "\n",
    "    location_ids = ts_data['pickup_location_id'].unique()\n",
    "    features = pd.DataFrame()\n",
    "    targets = pd.DataFrame()\n",
    "\n",
    "    for location_id in tqdm(location_ids):\n",
    "\n",
    "        # keep only ts data for this `location_id`\n",
    "        ts_data_one_location = ts_data.loc[\n",
    "            ts_data.pickup_location_id == location_id,\n",
    "            ['pickup_hour', 'number_of_rides']\n",
    "        ]\n",
    "\n",
    "        # pre-compute cutoff indices to split dataframe rows\n",
    "        indices = get_cutoff_indices(\n",
    "            ts_data_one_location,\n",
    "            input_seq_len,\n",
    "            step_size\n",
    "        )\n",
    "\n",
    "        # slice and transpose data into numpy arrays for features and targets\n",
    "        n_examples = len(indices)\n",
    "        x = np.ndarray(shape=(n_examples, input_seq_len), dtype=np.float32)\n",
    "        y = np.ndarray(shape=(n_examples), dtype=np.float32)\n",
    "        pickup_hours = []\n",
    "        for i, idx in enumerate(indices):\n",
    "            x[i, :] = ts_data_one_location.iloc[idx[0]:idx[1]]['number_of_rides'].values\n",
    "            y[i] = ts_data_one_location.iloc[idx[1]:idx[2]]['number_of_rides'].values\n",
    "            pickup_hours.append(ts_data_one_location.iloc[idx[1]]['pickup_hour'])\n",
    "\n",
    "        # numpy -> pandas\n",
    "        features_one_location = pd.DataFrame(\n",
    "            x,\n",
    "            columns=[f'rides_previous_{i+1}_hour' for i in reversed(range(input_seq_len))]\n",
    "        )\n",
    "        features_one_location['pickup_hour'] = pickup_hours\n",
    "        features_one_location['pickup_location_id'] = location_id\n",
    "\n",
    "        # numpy -> pandas\n",
    "        targets_one_location = pd.DataFrame(y, columns=[f'target_rides_next_hour'])\n",
    "\n",
    "        # concatenate results\n",
    "        features = pd.concat([features, features_one_location])\n",
    "        targets = pd.concat([targets, targets_one_location])\n",
    "\n",
    "    features.reset_index(inplace=True, drop=True)\n",
    "    targets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return features, targets['target_rides_next_hour']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IReqg9IxMYGl"
   },
   "outputs": [],
   "source": [
    "features, targets = transform_ts_data_into_features_and_target(\n",
    "    agg_rides_all_slots_may,\n",
    "    input_seq_len=24*27*1, # one week of history\n",
    "    step_size=24,\n",
    ")\n",
    "\n",
    "print(f'{features.shape=}')\n",
    "print(f'{targets.shape=}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUMNvyYMMYGl"
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5c-YTb8MYGl"
   },
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPBaNek6MYGl"
   },
   "outputs": [],
   "source": [
    "tabular_data_may = features\n",
    "tabular_data_may['target_rides_next_hour'] = targets\n",
    "\n",
    "tabular_data_may.to_parquet('tabular_data_may.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JD7e7rz_MYGm"
   },
   "outputs": [],
   "source": [
    "tabular_data_may\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3BN0nUUMYGm"
   },
   "source": [
    "### Preprocess for June - (will do this for every month and combine during visuallization(EDA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJ5alVOxMYGm"
   },
   "outputs": [],
   "source": [
    "rides_jun = pd.read_parquet('/Users/chetanhalai/Documents/code base for projects/4) uber/FINAL TAXI PROJECT 2023/data/2023 - JUN/yellow_tripdata_2023-06.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUTmraHmMYGm"
   },
   "outputs": [],
   "source": [
    "print(\"April month data shape:\", rides_jun.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRUjM-N-MYGm"
   },
   "outputs": [],
   "source": [
    "rides_jun.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ek4-PsSmMYGm"
   },
   "outputs": [],
   "source": [
    "rides_jun = rides_jun [['tpep_pickup_datetime', 'PULocationID' ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cltEWHp-MYGm"
   },
   "outputs": [],
   "source": [
    "rides_jun.rename(columns={'tpep_pickup_datetime': 'pickup_datetime',\n",
    "                      'PULocationID': 'pickup_location_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yt6CJ8YUMYGm"
   },
   "outputs": [],
   "source": [
    "rides_jun.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMbj8-z1MYGn"
   },
   "outputs": [],
   "source": [
    "rides_jun = rides_jun[rides_jun.pickup_datetime >= '2023-06-01']\n",
    "rides_jun = rides_jun[rides_jun.pickup_datetime < '2023-07-01']\n",
    "rides_jun ['pickup_datetime'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7InE8OEMYGn"
   },
   "outputs": [],
   "source": [
    "#Rounded data to the hour and will work with data in hourly intervals\n",
    "rides_jun['pickup_hour'] = rides_jun['pickup_datetime'].dt.floor('H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dsf8qmSkMYGn"
   },
   "outputs": [],
   "source": [
    "#going to group location ID and and pick up hour - (.size will give me the count of ride per hour - .reset_index goives me the data in ascending order)\n",
    "agg_rides_jun = rides_jun.groupby(['pickup_hour', 'pickup_location_id']).size().reset_index()\n",
    "agg_rides_jun.rename(columns={0: 'number_of_rides'}, inplace=True) #inplace=True is permenant\n",
    "agg_rides_jun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRBmkcDjMYGn"
   },
   "outputs": [],
   "source": [
    "agg_rides_jun.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UScxyy6VMYGn"
   },
   "outputs": [],
   "source": [
    "#this functions purpose is to fill in the data that was not present in the original data because there where no rides - time series data requires ther to be data in every time slot regardless of whether it existed or not\n",
    "from tqdm import tqdm\n",
    "\n",
    "def add_missing_slots(agg_rides: pd.DataFrame) -> pd.DataFrame: #take in the aggressed dataset that i presievely made and create an output a new DF which is going to be a complete timeseries\n",
    "\n",
    "    location_ids = agg_rides['pickup_location_id'].unique() # extract all the location ids and the full range of dates (bellow)...\n",
    "    full_range = pd.date_range(\n",
    "        agg_rides['pickup_hour'].min(), agg_rides['pickup_hour'].max(), freq='H') # ... from the first date till the last date and then building a series hour by hour\n",
    "    output = pd.DataFrame()\n",
    "    for location_id in tqdm(location_ids): #I did this operation per location ID .....\n",
    "\n",
    "        # keep only rides for this 'location_id'\n",
    "        agg_rides_i = agg_rides.loc[agg_rides.pickup_location_id == location_id, ['pickup_hour', 'number_of_rides']] #.... so i filter the data for onlyt that location ID and then we do this operatiom\n",
    "\n",
    "        # quick way to add missing dates with 0 in a Series\n",
    "        # taken from https://stackoverflow.com/a/19324591\n",
    "        agg_rides_i.set_index('pickup_hour', inplace=True) #this is the Reindex operation which basically takes in a df, (agg_rides_i), and then a full range of dates i want to have ...\n",
    "        agg_rides_i.index = pd.DatetimeIndex(agg_rides_i.index) #..... to ensure the out put df, has as many rows(and dates ) as the full range ......\n",
    "        agg_rides_i = agg_rides_i.reindex(full_range, fill_value=0) #... if thats not tne case it will fill it by 0)\n",
    "\n",
    "        # add back `location_id` columns\n",
    "        agg_rides_i['pickup_location_id'] = location_id #I then add back the location ID in this final dataframe\n",
    "\n",
    "        output = pd.concat([output, agg_rides_i]) # the i concatinate the results with the ones i got from the previous iterations and loop location id by location id\n",
    "\n",
    "    # move the purchase_day from the index to a dataframe column\n",
    "    output = output.reset_index().rename(columns={'index': 'pickup_hour'}) #then create a new column name, so this is why we need to add this rename column to inert thje index\n",
    "                                                                #which is now the datetime as a column and then rename it and its the pickup_hour\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JM2GVf-EMYGn"
   },
   "outputs": [],
   "source": [
    "print(agg_rides_jun.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JV7EniFMYGn"
   },
   "outputs": [],
   "source": [
    "#calling the function and adding the missing slots\n",
    "agg_rides_all_slots_jun = add_missing_slots(agg_rides_jun)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4d3e9Q1lMYGo"
   },
   "outputs": [],
   "source": [
    "#visualise the data to detect bigs in my data processing - check if the graph has any breaks in it\n",
    "from typing import Optional, List\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_rides(\n",
    "    rides: pd.DataFrame,\n",
    "    locations: Optional[List[int]] = None\n",
    "    ):     #this function takes in the dataframe i made previously (rides) and the optionally takes in the locations, the ones that will be plotted\n",
    "    \"\"\"\"\n",
    "    Plot time-series data\n",
    "    \"\"\"\n",
    "    rides_to_plot = rides[rides.pickup_location_id.isin(locations)] if locations else rides #once i filtered the data frame I just call the plotly dot line function and it will plot multiple lines\n",
    "\n",
    "    fig = px.line(\n",
    "        rides_to_plot,\n",
    "        x=\"pickup_hour\",\n",
    "        y=\"number_of_rides\",\n",
    "        color='pickup_location_id',\n",
    "        template='none',\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZB_bZdpuMYGo"
   },
   "outputs": [],
   "source": [
    "plot_rides(agg_rides_all_slots_jun, locations=[261])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpUWhZg-MYGo"
   },
   "outputs": [],
   "source": [
    "print('Number of pickup locations:', rides_jun.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0gQstaXMYGo"
   },
   "source": [
    "### time series - turning time into features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AqZSBDGOMYGo"
   },
   "outputs": [],
   "source": [
    "#The idea behind this function is to create overlapping windows of data from a time series,\n",
    "# which can be especially useful for problems like forecasting. By having a window of n_features data points predict the next data point,\n",
    "# we're essentially framing a forecasting problem as a supervised learning problem, allowing traditional machine learning models to be applied on time series data.\n",
    "#we trying to take partition out data in 1 hours interval across 24 hours - We took the indicies\n",
    "def get_cutoff_indices(\n",
    "    data: pd.DataFrame,\n",
    "    n_features: int,\n",
    "    step_size: int\n",
    "    ) -> list:\n",
    "\n",
    "        stop_position = len(data) - 1 # Defines the final index position in the data. This ensures that the sub-sequences we create don't overshoot the actual data length\n",
    "\n",
    "        # Start the first sub-sequence at index position 0 - mid_idx nfeatures\n",
    "        subseq_first_idx = 0\n",
    "        subseq_mid_idx = n_features\n",
    "        subseq_last_idx = n_features + 1   #This index denotes the position of our target data point.\n",
    "        indices = []\n",
    "\n",
    "        while subseq_last_idx <= stop_position:\n",
    "            indices.append((subseq_first_idx, subseq_mid_idx, subseq_last_idx))    #Continues creating new sub-sequences as long as the sub-sequence's end doesn't exceed the length of the data. - then each tuple in this list provides starting, middle and ending boundary for each bundary for a subsequence\n",
    "\n",
    "            subseq_first_idx += step_size\n",
    "            subseq_mid_idx += step_size\n",
    "            subseq_last_idx += step_size        #These lines progress the boundaries forward in the dataset by step_size, moving to the next overlapping window.\n",
    "\n",
    "        return indices                           #The function ends by returning the populated list of index boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzfxnIyzMYGo"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def transform_ts_data_into_features_and_target(\n",
    "    ts_data: pd.DataFrame,\n",
    "    input_seq_len: int,\n",
    "    step_size: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Slices and transposes data from time-series format into a (features, target)\n",
    "    format that we can use to train Supervised ML models\n",
    "    \"\"\"\n",
    "    assert set(ts_data.columns) == {'pickup_hour', 'number_of_rides', 'pickup_location_id'}\n",
    "\n",
    "    location_ids = ts_data['pickup_location_id'].unique()\n",
    "    features = pd.DataFrame()\n",
    "    targets = pd.DataFrame()\n",
    "\n",
    "    for location_id in tqdm(location_ids):\n",
    "\n",
    "        # keep only ts data for this `location_id`\n",
    "        ts_data_one_location = ts_data.loc[\n",
    "            ts_data.pickup_location_id == location_id,\n",
    "            ['pickup_hour', 'number_of_rides']\n",
    "        ]\n",
    "\n",
    "        # pre-compute cutoff indices to split dataframe rows\n",
    "        indices = get_cutoff_indices(\n",
    "            ts_data_one_location,\n",
    "            input_seq_len,\n",
    "            step_size\n",
    "        )\n",
    "\n",
    "        # slice and transpose data into numpy arrays for features and targets\n",
    "        n_examples = len(indices)\n",
    "        x = np.ndarray(shape=(n_examples, input_seq_len), dtype=np.float32)\n",
    "        y = np.ndarray(shape=(n_examples), dtype=np.float32)\n",
    "        pickup_hours = []\n",
    "        for i, idx in enumerate(indices):\n",
    "            x[i, :] = ts_data_one_location.iloc[idx[0]:idx[1]]['number_of_rides'].values\n",
    "            y[i] = ts_data_one_location.iloc[idx[1]:idx[2]]['number_of_rides'].values\n",
    "            pickup_hours.append(ts_data_one_location.iloc[idx[1]]['pickup_hour'])\n",
    "\n",
    "        # numpy -> pandas\n",
    "        features_one_location = pd.DataFrame(\n",
    "            x,\n",
    "            columns=[f'rides_previous_{i+1}_hour' for i in reversed(range(input_seq_len))]\n",
    "        )\n",
    "        features_one_location['pickup_hour'] = pickup_hours\n",
    "        features_one_location['pickup_location_id'] = location_id\n",
    "\n",
    "        # numpy -> pandas\n",
    "        targets_one_location = pd.DataFrame(y, columns=[f'target_rides_next_hour'])\n",
    "\n",
    "        # concatenate results\n",
    "        features = pd.concat([features, features_one_location])\n",
    "        targets = pd.concat([targets, targets_one_location])\n",
    "\n",
    "    features.reset_index(inplace=True, drop=True)\n",
    "    targets.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return features, targets['target_rides_next_hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJs2tR83MYGp"
   },
   "outputs": [],
   "source": [
    "features, targets = transform_ts_data_into_features_and_target(\n",
    "    agg_rides_all_slots_jun,\n",
    "    input_seq_len=24*27*1, # one week of history\n",
    "    step_size=24,\n",
    ")\n",
    "\n",
    "print(f'{features.shape=}')\n",
    "print(f'{targets.shape=}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4u-_okeMYGp"
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IVbQ28jWMYGp"
   },
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00rPWjMkMYGp"
   },
   "outputs": [],
   "source": [
    "tabular_data_jun = features\n",
    "tabular_data_jun['target_rides_next_hour'] = targets\n",
    "\n",
    "tabular_data_jun.to_parquet('tabular_data_jun.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yh07dkGbMYGp"
   },
   "outputs": [],
   "source": [
    "tabular_data_jun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking every months data for any defects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data_jan.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tabular_data_jan.shape)\n",
    "print(tabular_data_feb.shape)\n",
    "print(tabular_data_mar.shape)\n",
    "print(tabular_data_apr.shape)\n",
    "print(tabular_data_may.shape)\n",
    "print(tabular_data_jun.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data_feb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data_mar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data_apr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data_may.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data_jun.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating all time series data - (for hopsworks feature store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data_final = pd.concat([agg_rides_all_slots_jan, agg_rides_all_slots_feb, agg_rides_all_slots_mar, agg_rides_all_slots_apr, agg_rides_all_slots_may, agg_rides_all_slots_jun])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data_final.to_parquet('ts_data_final.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONCATENATING ALL DATAFRAMES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data_final = pd.concat([tabular_data_jan, tabular_data_feb, tabular_data_mar, tabular_data_apr, tabular_data_may, tabular_data_jun], ignore_index = True)\n",
    "\n",
    "tabular_data_final[760:780]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_data_final.to_parquet('tabular_data_final.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning clarity \n",
    "\n",
    "columns = features \n",
    "varibales and dimensions \n",
    "\n",
    "target_rides_next_hour = dependendent varibales - target varibale \n",
    "\n",
    "rows = observation \n",
    "\n",
    "eg - first row - ( on 2023-01-28 at pick up location id 4 at the first hour there were 22rides and on the 2 hour there were 18 rides )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "74c9646b64cee09a16e842b01c20df6c5e73a666ad2628af3aba37ea15d5eeaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
